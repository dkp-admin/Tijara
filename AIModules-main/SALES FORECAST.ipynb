{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10927730,"sourceType":"datasetVersion","datasetId":6794248}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from pymongo import MongoClient\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom datetime import datetime\n\n# Connect to MongoDB\nclient = MongoClient(\"mongodb://user12:dpu3rN4Uhq@193.122.67.202:27088/tijarah?authSource=tijarah\")\ndb = client[\"tijarah\"]\norders_collection = db[\"orders\"]\n\ndef get_sales_data():\n    \"\"\"\n    Fetch and flatten sales data from MongoDB.\n    \"\"\"\n    pipeline = [\n        {\"$match\": {\"items.name.en\": {\"$ne\": \"Open Item\"}}},  # Exclude unwanted items\n        {\"$unwind\": \"$items\"},  # Flatten the 'items' array\n        {\"$project\": {  # Select relevant fields (use .get() to prevent missing key errors)\n            \"date\": {\"$dateToString\": {\"format\": \"%Y-%m-%d\", \"date\": \"$updatedAt\"}},\n            \"companyName\": \"$company.name\",\n            \"companyRef\": \"$companyRef\",\n            \"location\": \"$location.name\",\n            \"productName\": \"$items.name.en\",\n            \"productRef\": \"$items.productRef\",\n            \"quantity\": {\"$ifNull\": [\"$items.quantity\", 0]},\n            \"total_sales\": {\"$ifNull\": [\"$items.billing.total\", 0]},\n            \"sellingPrice\": {\"$ifNull\": [\"$items.sellingPrice\", 0]},\n            \"costPrice\": {\"$ifNull\": [\"$items.costPrice\", 0]},\n            \"vatAmount\": {\"$ifNull\": [\"$payment.vatAmount\", 0]},\n            \"discountAmount\": {\"$ifNull\": [\"$payment.discountAmount\", 0]},\n            \"paymentType\": {\"$arrayElemAt\": [\"$payment.breakup.name\", 0]}\n        }}\n    ]\n    cursor = orders_collection.aggregate(pipeline, allowDiskUse=True)\n    return list(cursor)  # Convert cursor to list\n\ndef preprocess_sales_data():\n    \"\"\"\n    Cleans and processes sales data with missing value handling, resampling, and scaling.\n    \"\"\"\n    sales_data = get_sales_data()\n    df = pd.DataFrame(sales_data)\n\n    if df.empty:\n        return pd.DataFrame()  # Return empty DataFrame if no data\n\n    # Print column names to debug missing column issue\n    print(\"Columns in DataFrame:\", df.columns)\n\n    # Convert 'date' column to datetime format\n    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n\n    # Drop rows with missing dates\n    df = df.dropna(subset=['date'])\n\n    # Convert numeric columns to float/int\n    numeric_cols = ['quantity', 'total_sales', 'sellingPrice', 'costPrice', 'vatAmount', 'discountAmount']\n\n    # Ensure all required columns exist before applying transformations\n    for col in numeric_cols:\n        if col not in df.columns:\n            df[col] = 0  # Create missing columns with default value\n\n    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n\n    # Drop columns where more than 50% values are missing\n    threshold = len(df) * 0.5\n    df = df.dropna(thresh=threshold, axis=1)\n\n    # Ensure 'date' is the index before resampling\n    df = df.set_index('date')\n\n    # Preserve company names before resampling\n    df = df.sort_index()\n    df['companyName'] = df.groupby('companyRef')['companyName'].ffill()\n\n    # Resample to daily frequency and fill missing values\n    df_resampled = df.groupby(['companyRef', 'companyName']).resample('D').sum(numeric_only=True).reset_index()\n\n    # Forward-fill missing values\n    df_resampled[numeric_cols] = df_resampled[numeric_cols].ffill()\n\n    # Interpolate missing sales values\n    df_resampled[numeric_cols] = df_resampled[numeric_cols].interpolate(method='linear')\n\n    # Outlier Detection using Interquartile Range (IQR)\n    Q1 = df_resampled['total_sales'].quantile(0.25)\n    Q3 = df_resampled['total_sales'].quantile(0.75)\n    IQR = Q3 - Q1\n\n    # Define lower and upper bounds\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n\n    # Cap outliers\n    df_resampled['total_sales'] = np.where(df_resampled['total_sales'] < lower_bound, lower_bound, df_resampled['total_sales'])\n    df_resampled['total_sales'] = np.where(df_resampled['total_sales'] > upper_bound, upper_bound, df_resampled['total_sales'])\n\n    # Apply MinMax Scaling (Normalize between 0 and 1)\n    scaler = MinMaxScaler()\n    df_resampled['scaled_sales'] = scaler.fit_transform(df_resampled[['total_sales']])\n\n    return df_resampled\n\n# Preprocess the data\ndf = preprocess_sales_data()\n\n# Display first few rows\nprint(df.head())\n\n# Save the entire dataset as a CSV file\ndf.to_csv(\"preprocessed_sales_data.csv\", index=False)\n\nprint(\"âœ… Data saved as 'preprocessed_sales_data.csv'\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T05:26:45.421379Z","iopub.execute_input":"2025-03-12T05:26:45.421764Z","iopub.status.idle":"2025-03-12T05:34:05.339411Z","shell.execute_reply.started":"2025-03-12T05:26:45.421732Z","shell.execute_reply":"2025-03-12T05:34:05.338336Z"}},"outputs":[{"name":"stdout","text":"Columns in DataFrame: Index(['_id', 'date', 'companyName', 'companyRef', 'location', 'productName',\n       'productRef', 'quantity', 'total_sales', 'sellingPrice', 'costPrice',\n       'vatAmount', 'discountAmount', 'paymentType'],\n      dtype='object')\n                 companyRef                companyName       date  quantity  \\\n0  64e5b0095557ec59100f09b2               ELV pvt. ltd 2023-09-27       5.0   \n1  6509ba0d65dc08019b0b0c79  T Mart - For Test Purpose 2024-01-11       6.0   \n2  6509ba0d65dc08019b0b0c79  T Mart - For Test Purpose 2024-01-12       5.0   \n3  6509ba0d65dc08019b0b0c79  T Mart - For Test Purpose 2024-01-13       0.0   \n4  6509ba0d65dc08019b0b0c79  T Mart - For Test Purpose 2024-01-14       0.0   \n\n   total_sales  sellingPrice  costPrice  vatAmount  discountAmount  \\\n0       766.25             0          0     266.10             0.0   \n1       506.89             0          0      66.12             0.0   \n2       100.00             0          0      13.04             0.0   \n3         0.00             0          0       0.00             0.0   \n4         0.00             0          0       0.00             0.0   \n\n   scaled_sales  \n0      1.000000  \n1      0.661520  \n2      0.130506  \n3      0.000000  \n4      0.000000  \nâœ… Data saved as 'preprocessed_sales_data.csv'\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\n# Load preprocessed data\ndf = preprocess_sales_data()\n\n# Drop company name (since we have companyRef)\ndf = df.drop(columns=['companyName'])\n\n# One-hot encode companyRef\ndf = pd.get_dummies(df, columns=['companyRef'])\n\n# Scale numeric values (total_sales)\nscaler = MinMaxScaler()\ndf['total_sales'] = scaler.fit_transform(df[['total_sales']])\n\nprint(df.head())  # Inspect transformed data\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T05:35:36.846086Z","iopub.execute_input":"2025-03-12T05:35:36.846508Z","iopub.status.idle":"2025-03-12T05:40:43.614072Z","shell.execute_reply.started":"2025-03-12T05:35:36.846477Z","shell.execute_reply":"2025-03-12T05:40:43.612867Z"}},"outputs":[{"name":"stdout","text":"Columns in DataFrame: Index(['_id', 'date', 'companyName', 'companyRef', 'location', 'productName',\n       'productRef', 'quantity', 'total_sales', 'sellingPrice', 'costPrice',\n       'vatAmount', 'discountAmount', 'paymentType'],\n      dtype='object')\n        date  quantity  total_sales  sellingPrice  costPrice  vatAmount  \\\n0 2023-09-27       5.0     1.000000             0          0     266.10   \n1 2024-01-11       6.0     0.661520             0          0      66.12   \n2 2024-01-12       5.0     0.130506             0          0      13.04   \n3 2024-01-13       0.0     0.000000             0          0       0.00   \n4 2024-01-14       0.0     0.000000             0          0       0.00   \n\n   discountAmount  scaled_sales  companyRef_64e5b0095557ec59100f09b2  \\\n0             0.0      1.000000                                 True   \n1             0.0      0.661520                                False   \n2             0.0      0.130506                                False   \n3             0.0      0.000000                                False   \n4             0.0      0.000000                                False   \n\n   companyRef_6509ba0d65dc08019b0b0c79  ...  \\\n0                                False  ...   \n1                                 True  ...   \n2                                 True  ...   \n3                                 True  ...   \n4                                 True  ...   \n\n   companyRef_676bf160b5d835cdab06ef95  companyRef_676bf40cb5d835cdab06f383  \\\n0                                False                                False   \n1                                False                                False   \n2                                False                                False   \n3                                False                                False   \n4                                False                                False   \n\n   companyRef_676d55acb5d835cdab08f93a  companyRef_6773b35cb5d835cdab0ebd7e  \\\n0                                False                                False   \n1                                False                                False   \n2                                False                                False   \n3                                False                                False   \n4                                False                                False   \n\n   companyRef_6774f56fb5d835cdab1131f5  companyRef_678ccc0cb1e5f59c6264b63f  \\\n0                                False                                False   \n1                                False                                False   \n2                                False                                False   \n3                                False                                False   \n4                                False                                False   \n\n   companyRef_6798cbe1ba4ab1d45728f9b9  companyRef_679d3752ba4ab1d4572ce55f  \\\n0                                False                                False   \n1                                False                                False   \n2                                False                                False   \n3                                False                                False   \n4                                False                                False   \n\n   companyRef_67a201f60e2b3511b036ea73  companyRef_67c0431be37caf59867541dd  \n0                                False                                False  \n1                                False                                False  \n2                                False                                False  \n3                                False                                False  \n4                                False                                False  \n\n[5 rows x 115 columns]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n\n# Define sequence length (number of past days to consider for forecasting)\nsequence_length = 14  # Using the last 14 days to predict the next day's sales\n\n# Extract features (X) and target (y)\nfeatures = df.drop(columns=['date', 'total_sales']).values\ntarget = df['total_sales'].values\n\n# Create LSTM input sequences\ngenerator = TimeseriesGenerator(features, target, length=sequence_length, batch_size=32)\n\n# Inspect one batch\nx_batch, y_batch = generator[0]\nprint(f\"Shape of X: {x_batch.shape}, Shape of y: {y_batch.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T05:41:44.397219Z","iopub.execute_input":"2025-03-12T05:41:44.397656Z","iopub.status.idle":"2025-03-12T05:41:58.692650Z","shell.execute_reply.started":"2025-03-12T05:41:44.397623Z","shell.execute_reply":"2025-03-12T05:41:58.691564Z"}},"outputs":[{"name":"stdout","text":"Shape of X: (32, 14, 113), Shape of y: (32,)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Input\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nfrom sklearn.preprocessing import MinMaxScaler\n\n# **Step 1: Load Preprocessed Data**\ndf = preprocess_sales_data()\n\n# **Step 2: Ensure All Features Are Numeric**\ndf = df.drop(columns=['companyName'])  # Drop unnecessary columns\ndf = pd.get_dummies(df, columns=['companyRef'])  # One-hot encode companyRef\n\n# **Step 3: Normalize Sales Data**\nscaler = MinMaxScaler()\ndf['total_sales'] = scaler.fit_transform(df[['total_sales']])\n\n# **Step 4: Convert Data into LSTM Format**\nsequence_length = 14  # Use last 14 days of data for predictions\n\n# Extract features (X) and target (y)\nfeatures = df.drop(columns=['date']).values  # Drop date column\ntarget = df['total_sales'].values\n\n# Convert to float to prevent dtype errors\nfeatures = features.astype(np.float32)\ntarget = target.astype(np.float32)\n\n# Create sequences for LSTM\ngenerator = TimeseriesGenerator(features, target, length=sequence_length, batch_size=32)\n\n# **Step 5: Build and Train the LSTM Model**\nmodel = Sequential([\n    Input(shape=(sequence_length, features.shape[1])),  # Define input shape\n    LSTM(50, activation='relu', return_sequences=True),\n    Dropout(0.2),\n    LSTM(50, activation='relu'),\n    Dropout(0.2),\n    Dense(1)  # Output layer (predicting total_sales)\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mse')\n\n# Train the model\nmodel.fit(generator, epochs=20, verbose=1)\n\nprint(\"âœ… LSTM Model Trained Successfully!\")\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\n\n# Build LSTM Model\nmodel = Sequential([\n    LSTM(50, activation='relu', return_sequences=True, input_shape=(sequence_length, features.shape[1])),\n    Dropout(0.2),\n    LSTM(50, activation='relu'),\n    Dropout(0.2),\n    Dense(1)  # Predicting total_sales\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mse')\n\n# Train the model\nmodel.fit(generator, epochs=20, verbose=1)\n\nprint(\"âœ… LSTM Model Trained!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T05:42:05.577874Z","iopub.execute_input":"2025-03-12T05:42:05.578629Z","iopub.status.idle":"2025-03-12T05:55:37.483589Z","shell.execute_reply.started":"2025-03-12T05:42:05.578596Z","shell.execute_reply":"2025-03-12T05:55:37.482155Z"}},"outputs":[{"name":"stdout","text":"Columns in DataFrame: Index(['_id', 'date', 'companyName', 'companyRef', 'location', 'productName',\n       'productRef', 'quantity', 'total_sales', 'sellingPrice', 'costPrice',\n       'vatAmount', 'discountAmount', 'paymentType'],\n      dtype='object')\nEpoch 1/20\n\u001b[1m  4/676\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - loss: 106628.2891","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 17ms/step - loss: 2165081856.0000\nEpoch 2/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 1083469440.0000\nEpoch 3/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 114003152.0000\nEpoch 4/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 68818720.0000\nEpoch 5/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 20311038.0000\nEpoch 6/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 4230564.5000\nEpoch 7/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 17322796.0000\nEpoch 8/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 56602796.0000\nEpoch 9/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 14838952.0000\nEpoch 10/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 15601102.0000\nEpoch 11/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - loss: 27812412.0000\nEpoch 12/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 14757544.0000\nEpoch 13/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 1310873.2500\nEpoch 14/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 277326.5312\nEpoch 15/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 376848.8438\nEpoch 16/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 1978471.1250\nEpoch 17/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 3316364.0000\nEpoch 18/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 9132224.0000\nEpoch 19/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 1209757.1250\nEpoch 20/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 63450.3672\nâœ… LSTM Model Trained Successfully!\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - loss: 255016816.0000\nEpoch 2/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 192120816.0000\nEpoch 3/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 185442976.0000\nEpoch 4/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 77857992.0000\nEpoch 5/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 316664448.0000\nEpoch 6/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 40220900.0000\nEpoch 7/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 1592942.6250\nEpoch 8/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 70473888.0000\nEpoch 9/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 4711606.0000\nEpoch 10/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - loss: 3584550.2500\nEpoch 11/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 3480087.0000\nEpoch 12/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 9132585.0000\nEpoch 13/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 1449072.6250\nEpoch 14/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 198603.4844\nEpoch 15/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 205841.8594\nEpoch 16/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 1198640.8750\nEpoch 17/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 132836.0625\nEpoch 18/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 59866.1016\nEpoch 19/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 545115.3125\nEpoch 20/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 12175.3926\nâœ… LSTM Model Trained!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"model.fit(generator, epochs=20, verbose=1)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T05:56:11.037708Z","iopub.execute_input":"2025-03-12T05:56:11.038106Z","iopub.status.idle":"2025-03-12T06:00:05.826247Z","shell.execute_reply.started":"2025-03-12T05:56:11.038076Z","shell.execute_reply":"2025-03-12T06:00:05.824816Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 5156.7524\nEpoch 2/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 22559.7539\nEpoch 3/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 1129270.8750\nEpoch 4/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 32506.1836\nEpoch 5/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 3835.9888\nEpoch 6/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 291266.2812\nEpoch 7/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - loss: 2738.3396\nEpoch 8/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - loss: 2501.9390\nEpoch 9/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - loss: 161742.9531\nEpoch 10/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 18ms/step - loss: 12707.6592\nEpoch 11/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 27108.3809\nEpoch 12/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 2566.5281\nEpoch 13/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 5849.7090\nEpoch 14/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 1311.8479\nEpoch 15/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 2207.9446\nEpoch 16/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 539.7497\nEpoch 17/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 1338.4949\nEpoch 18/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 97.4612\nEpoch 19/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 68481.7344\nEpoch 20/20\n\u001b[1m676/676\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - loss: 0.0795\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7a8c54f13dc0>"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"**ROBUST SCALAR 50EPOCHS**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom ipywidgets import Dropdown, Output, VBox, Label\nfrom IPython.display import display\n\n# Load and Prepare Data\ndf = pd.read_csv(\"preprocessed_sales_data.csv\", parse_dates=['date'])\n\ndef remove_outliers(df):\n    \"\"\"\n    Removes outliers from the 'total_sales' column using the IQR method.\n    \"\"\"\n    Q1 = df['total_sales'].quantile(0.25)\n    Q3 = df['total_sales'].quantile(0.75)\n    IQR = Q3 - Q1\n    \n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    df = df[(df['total_sales'] >= lower_bound) & (df['total_sales'] <= upper_bound)]\n    return df\n\ndef get_company_data(df, company_ref):\n    \"\"\"\n    Filters the sales data for a specific company, removes outliers, and applies smoothing.\n    \"\"\"\n    company_data = df[df['companyRef'] == company_ref][['date', 'total_sales']].copy()\n    \n    if company_data.empty or len(company_data) < 30:\n        print(f\"âš ï¸ Not enough data for {company_ref}. Skipping forecasting.\")\n        return None\n    \n    company_data = remove_outliers(company_data)  # Remove outliers before processing\n    company_data['total_sales'] = company_data['total_sales'].rolling(window=3, min_periods=1).mean()\n    company_data.set_index('date', inplace=True)\n    return company_data\n\ndef train_lstm(df, window_size=14, epochs=50):\n    \"\"\"\n    Trains an LSTM model on the company's sales data.\n    \"\"\"\n    scaler = RobustScaler()\n    df_scaled = scaler.fit_transform(df[['total_sales']])\n\n    X, y = [], []\n    for i in range(window_size, len(df_scaled)):\n        X.append(df_scaled[i-window_size:i])\n        y.append(df_scaled[i])\n\n    X, y = np.array(X), np.array(y)\n    \n    train_size = int(len(X) * 0.8)\n    X_train, X_test = X[:train_size], X[train_size:]\n    y_train, y_test = y[:train_size], y[train_size:]\n\n    model = Sequential([\n        LSTM(64, activation='tanh', return_sequences=True, input_shape=(window_size, 1)),\n        Dropout(0.2),\n        LSTM(64, activation='tanh'),\n        Dropout(0.2),\n        Dense(1)\n    ])\n    model.compile(optimizer='adam', loss='mse')\n\n    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n    model.fit(X_train, y_train, epochs=epochs, batch_size=32, validation_data=(X_test, y_test), verbose=1, callbacks=[early_stopping])\n\n    y_pred = model.predict(X_test)\n    y_pred_inv = scaler.inverse_transform(y_pred)\n    y_test_inv = scaler.inverse_transform(y_test)\n\n    mae = mean_absolute_error(y_test_inv, y_pred_inv)\n    accuracy = max(0, 100 - (mae / np.mean(y_test_inv) * 100))\n    print(f\"âœ… LSTM Model Accuracy: {accuracy:.2f}%\")\n    \n    return model, scaler\n\ndef forecast_sales_lstm(model, df, scaler, window_size=14):\n    \"\"\"\n    Generates a 7-day sales forecast using the trained LSTM model.\n    \"\"\"\n    future_sales = []\n    last_sequence = df['total_sales'].values[-window_size:]\n\n    for _ in range(7):\n        input_seq = np.array(last_sequence).reshape(-1, 1)\n        input_seq = scaler.transform(input_seq)\n        input_seq = input_seq.reshape(1, window_size, 1)\n        predicted_sales = model.predict(input_seq, verbose=0)[0, 0]\n        predicted_sales = scaler.inverse_transform([[predicted_sales]])[0, 0]\n\n        future_sales.append(predicted_sales)\n        last_sequence = np.append(last_sequence[1:], predicted_sales)\n    \n    return future_sales\n\ndef baseline_forecast(df):\n    \"\"\"\n    Computes a simple moving average (SMA) forecast as a baseline.\n    \"\"\"\n    return df['total_sales'].rolling(window=7).mean().shift(-7).dropna().values[-7:]\n\ndef display_company_selector(df):\n    \"\"\"\n    Creates an interactive dropdown menu to select a company.\n    \"\"\"\n    unique_companies = df[['companyRef', 'companyName']].drop_duplicates()\n    if unique_companies.empty:\n        print(\"No companies have enough data for forecasting.\")\n        return\n    \n    company_dropdown = Dropdown(options={row['companyName']: row['companyRef'] for _, row in unique_companies.iterrows()}, description=\"Select Company:\")\n    output = Output()\n    \n    def update_forecast(change):\n        output.clear_output()\n        with output:\n            selected_company = company_dropdown.value\n            company_data = get_company_data(df, selected_company)\n            \n            if company_data is not None:\n                print(f\"ğŸ“Š Training LSTM for {company_dropdown.label}...\")\n                model, scaler = train_lstm(company_data)\n                future_sales = forecast_sales_lstm(model, company_data, scaler)\n                baseline_sales = baseline_forecast(company_data)\n                \n                days = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n                plt.figure(figsize=(10, 5))\n                plt.plot(days, future_sales, marker='o', linestyle='-', color='b', label=\"LSTM Predicted Sales\")\n                plt.plot(days, baseline_sales, marker='s', linestyle='--', color='r', label=\"Baseline (SMA) Forecast\")\n                plt.xlabel(\"Day\")\n                plt.ylabel(\"Total Sales\")\n                plt.title(f\"LSTM vs. Baseline Sales Forecast ({company_dropdown.label})\")\n                plt.legend()\n                plt.grid()\n                plt.show()\n    \n    company_dropdown.observe(update_forecast, names='value')\n    display(VBox([Label(\"Choose a company for forecasting:\"), company_dropdown, output]))\n\ndisplay_company_selector(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T06:54:23.692054Z","iopub.execute_input":"2025-03-12T06:54:23.692508Z","iopub.status.idle":"2025-03-12T06:54:23.793669Z","shell.execute_reply.started":"2025-03-12T06:54:23.692476Z","shell.execute_reply":"2025-03-12T06:54:23.792279Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Choose a company for forecasting:'), Dropdown(description='Select Company:', optioâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38c219e321a544d191725561d5f6df3f"}},"metadata":{}}],"execution_count":18}]}